# -*- coding: utf-8 -*-
"""Cuda_Mul.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18GfFspTcgObT4pHkizOkLybf7WLOa5Kk
"""

!/usr/local/cuda/bin/nvcc --version

!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git

# Commented out IPython magic to ensure Python compatibility.
# %load_ext nvcc_plugin

# Commented out IPython magic to ensure Python compatibility.
# %%cu
# #include <cuda.h>
# #include <stdio.h>
# #include <stdlib.h>
# #include <time.h>
# #define m 10
# __global__ void mul_r(int *a, int *b, int *c)
# {
#  	int tid = threadIdx.x;
#  	if (tid < m)
#   {
#  		c[tid]= a[tid] * b[tid];
#  	}
# }
# int main()
# {
#  	int  n, c, d, fst[10][10], snd[10][10], t_snd[10][10];
#  	int row,col,sum_c, a[10], b[10], ans[10];
#   n=m;
#   for (c = 0; c < m; c++)
#   {
#     for (d = 0; d < n; d++)
#  	  {
#  		 fst[c][d]=rand()%10+1;
#  		}
#  	}
#  	printf("display the elements of first matrix\n");	 
#  	for (c = 0; c < m; c++) 
#   {
#     for (d = 0 ; d < n; d++) 
#     {
#  			printf("%d\t", fst[c][d]);
#  		}
#  		printf("\n");	 
#  	}
#   for (c = 0; c < m; c++)
#   { 
#     for (d = 0; d < n; d++)
#  	  {
#  		 snd[c][d]=rand()%10+1;
#  		}
#  	}
#  	printf("display the elements of second matrix\n");	 
#  	for (c = 0; c < m; c++) 
#   {
#     for (d = 0 ; d < n; d++) 
#     {
#  			printf("%d\t", snd[c][d]);
#  		}
#  		printf("\n");	 
#  	}
#  	for(c=0; c<m; c++)
#   for(d=0; d<n; d++)
#   {
#     t_snd[d][c] = snd[c][d];
#   }
#   printf("\nTranspose of second Matrix:\n");
#   for (c = 0; c < n; c++) 
#   {
#     for (d = 0 ; d < m; d++) 
#     {
#  			printf("%d\t", t_snd[c][d]);
#  		}
#  		printf("\n");	 
#  	}
#  	int *dev_a, *dev_b,*dev_ans;
#  	cudaError_t err=cudaSuccess;
#  	err=cudaMalloc((void**)&dev_a,m * sizeof(int)); 
#  	if (err !=cudaSuccess)
#  	{ 	
#   printf("failed to  allocate on device \n");
#  	printf("error is:\n",cudaGetErrorString(err));
#  	exit(EXIT_FAILURE);
#  	}
#  	cudaMalloc((void**)&dev_b,m * sizeof(int)); 
#  	cudaMalloc((void**)&dev_ans,m * sizeof(int)); 
#  	row=0;
#  	col=0;
#   cudaEvent_t start, end;
#   cudaEventCreate(&start);
#   cudaEventCreate(&end);
#   cudaEventRecord(start);	  
#  	for(row=0; row<m; row++)
#   {	
#  	 for (d = 0 ; d < m; d++) 
#    {
#  		a[d]=fst[row][d];
#  	 }
#  	 cudaMemcpy(dev_a,a,m*sizeof(int), cudaMemcpyHostToDevice);	
#  	 for (col=0; col<m; col++)
#    {	
#  	  for (d= 0 ; d < m; d++) 
#     {
#  			b[d]=t_snd[col][d];
#  			ans[d]=0;
#  		}
#  		cudaMemcpy(dev_b,b,m*sizeof(int), cudaMemcpyHostToDevice);	
#  		cudaMemcpy(dev_ans,ans,m*sizeof(int), cudaMemcpyHostToDevice);
#  		mul_r<<<1,m>>>(dev_a,dev_b,dev_ans);
#  		err=cudaMemcpy(ans,dev_ans,m*sizeof(int), cudaMemcpyDeviceToHost);
#  		if (err !=cudaSuccess)
#  		{ 	
#         printf("failed to  copy from device \n");
#  				exit(EXIT_FAILURE);
#  		}
#  	  sum_c=0;
#  	  for (d = 0 ; d < m; d++) 
#     {
#  				sum_c+=ans[d];
#  		}
#  	  snd[row][col]=sum_c; 
#  	 }
#   }
#  	cudaEventRecord(end);
#   cudaEventSynchronize(end);
#   float time = 0;
#   cudaEventElapsedTime(&time, start, end);
#   printf("execution time=%f\n",time);
#  	printf(" Matrix multipliation ans=:\n");
#   for (c = 0; c < n; c++) 
#   {
#     for (d = 0 ; d < m; d++) 
#     {
#  			printf("%d\t", snd[c][d]);
#  		}
#  		printf("\n");	 
#  	}
#  	return 0;
# }

